{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iSe7iefFLm2",
        "outputId": "03c3164c-e0da-4264-ade1-fcca249a035a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 229 kB in 2s (109 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "spark_version = 'spark-3.4.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop3.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FBQ03e9HsaU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xyDOtmdFcm_",
        "outputId": "f6f492bf-cc18-4845-a13d-5c8d0933f8ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                 Open       High        Low      Close  Adj Close  Volume\n",
            "Date                                                                     \n",
            "2007-01-22   8.000000  13.333333   8.000000  13.333333  13.333333   15075\n",
            "2007-01-23  15.000000  15.000000  10.000000  10.600000  10.600000    9330\n",
            "2007-01-24  10.666667  15.000000  10.666667  11.933333  11.933333   20415\n",
            "2007-01-25  12.333333  13.400000  12.333333  13.333333  13.333333   12300\n",
            "2007-01-26  12.800000  12.800000  12.666667  12.733333  12.733333   16275\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#importing yfinance data\n",
        "import yfinance as yf\n",
        "celh = yf.download(\"CELH\")\n",
        "print(celh.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5y4mDUdIton"
      },
      "outputs": [],
      "source": [
        "#create a sparksession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"celh\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXQbuOX9Nfkj",
        "outputId": "9e07ee83-26fd-4578-f1d5-cdba7352a7dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------+\n",
            "|index|               Date|              Open|              High|               Low|             Close|         Adj Close|Volume|\n",
            "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------+\n",
            "|    0|2007-01-22 00:00:00|               8.0|13.333333015441895|               8.0|13.333333015441895|13.333333015441895| 15075|\n",
            "|    1|2007-01-23 00:00:00|              15.0|              15.0|              10.0|10.600000381469727|10.600000381469727|  9330|\n",
            "|    2|2007-01-24 00:00:00|10.666666984558105|              15.0|10.666666984558105|11.933333396911621|11.933333396911621| 20415|\n",
            "|    3|2007-01-25 00:00:00|12.333333015441895|13.399999618530273|12.333333015441895|13.333333015441895|13.333333015441895| 12300|\n",
            "|    4|2007-01-26 00:00:00|12.800000190734863|12.800000190734863|12.666666984558105|12.733332633972168|12.733332633972168| 16275|\n",
            "|    5|2007-01-29 00:00:00|              13.0|              16.0|              13.0|              16.0|              16.0| 54105|\n",
            "|    6|2007-01-30 00:00:00|16.333332061767578|18.666667938232422|16.133333206176758|              18.0|              18.0| 72885|\n",
            "|    7|2007-01-31 00:00:00|18.200000762939453|18.666667938232422| 17.46666717529297|18.666667938232422|18.666667938232422| 72705|\n",
            "|    8|2007-02-01 00:00:00|18.866666793823242|21.866666793823242|18.866666793823242|21.666667938232422|21.666667938232422|273735|\n",
            "|    9|2007-02-02 00:00:00|              22.0|22.866666793823242|19.333332061767578|21.066667556762695|21.066667556762695|167265|\n",
            "+-----+-------------------+------------------+------------------+------------------+------------------+------------------+------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#creating df in spark\n",
        "pandas_df = celh\n",
        "pandas_df.reset_index(drop=False,inplace=True)\n",
        "df = spark.createDataFrame(pandas_df)\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ql50CJXxRPe4"
      },
      "outputs": [],
      "source": [
        "#creating temp view\n",
        "df.createOrReplaceTempView('celh')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRWWG0pvOXXS",
        "outputId": "7eb4e642-b558-4a72-adc5-da8dc26e1fd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------------------+------------------+------------------+------------------+------------------+------------------+-------+\n",
            "|               Date|              Open|              High|               Low|             Close|         Adj Close| Volume|\n",
            "+-------------------+------------------+------------------+------------------+------------------+------------------+-------+\n",
            "|2020-01-02 00:00:00|1.6100000143051147|1.6233329772949219|1.5700000524520874|1.5833330154418945|1.5833330154418945| 858300|\n",
            "|2020-01-03 00:00:00|1.5666669607162476|1.5666669607162476|1.5099999904632568|1.5499999523162842|1.5499999523162842| 876000|\n",
            "|2020-01-06 00:00:00|1.5266669988632202| 1.553333044052124|1.4966670274734497|1.5066670179367065|1.5066670179367065| 804600|\n",
            "|2020-01-07 00:00:00|               1.5|               1.5|1.3700000047683716|1.4199999570846558|1.4199999570846558|3549300|\n",
            "|2020-01-08 00:00:00|1.4233330488204956|1.4833329916000366|1.4199999570846558| 1.463333010673523| 1.463333010673523|1061100|\n",
            "|2020-01-09 00:00:00|1.4833329916000366|1.5800000429153442|1.4566669464111328|1.5633330345153809|1.5633330345153809|1276500|\n",
            "|2020-01-10 00:00:00|1.5666669607162476|1.6299999952316284|1.5199999809265137|1.5399999618530273|1.5399999618530273| 717300|\n",
            "|2020-01-13 00:00:00|1.5399999618530273|1.6233329772949219|1.5199999809265137|1.6033329963684082|1.6033329963684082|1026600|\n",
            "|2020-01-14 00:00:00| 1.600000023841858|1.6233329772949219|1.5800000429153442|1.6033329963684082|1.6033329963684082| 577800|\n",
            "|2020-01-15 00:00:00|1.6166670322418213|1.6299999952316284|1.5933330059051514|1.6100000143051147|1.6100000143051147| 584100|\n",
            "+-------------------+------------------+------------------+------------------+------------------+------------------+-------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#query to find all dates between Jan 1st, 2020 and March 1st, 2024\n",
        "all_data = spark.sql(\"\"\"select * from celh where Date between \"2020-01-01\" and \"2024-03-01\" \"\"\").show(10)\n",
        "all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULBq3F-SRCvL",
        "outputId": "8287049d-7003-4d99-bae2-516d0d8315c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------------+\n",
            "|             Close|               Date|\n",
            "+------------------+-------------------+\n",
            "|1.5833330154418945|2020-01-02 00:00:00|\n",
            "|1.5499999523162842|2020-01-03 00:00:00|\n",
            "|1.5066670179367065|2020-01-06 00:00:00|\n",
            "|1.4199999570846558|2020-01-07 00:00:00|\n",
            "| 1.463333010673523|2020-01-08 00:00:00|\n",
            "|1.5633330345153809|2020-01-09 00:00:00|\n",
            "|1.5399999618530273|2020-01-10 00:00:00|\n",
            "|1.6033329963684082|2020-01-13 00:00:00|\n",
            "|1.6033329963684082|2020-01-14 00:00:00|\n",
            "|1.6100000143051147|2020-01-15 00:00:00|\n",
            "+------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#query to find only all closes between Jan 1st, 2020 and March 1st, 2024\n",
        "closes = spark.sql(\"\"\"select Close, Date from celh where Date between \"2020-01-01\" and \"2024-03-01\" \"\"\").show(10)\n",
        "closes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkkO60-WRpGp",
        "outputId": "8bf73816-9e7f-4032-dbf4-f0ee0984ea96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------------+-------------------+\n",
            "|         Adj Close|               Date|\n",
            "+------------------+-------------------+\n",
            "|1.5833330154418945|2020-01-02 00:00:00|\n",
            "|1.5499999523162842|2020-01-03 00:00:00|\n",
            "|1.5066670179367065|2020-01-06 00:00:00|\n",
            "|1.4199999570846558|2020-01-07 00:00:00|\n",
            "| 1.463333010673523|2020-01-08 00:00:00|\n",
            "|1.5633330345153809|2020-01-09 00:00:00|\n",
            "|1.5399999618530273|2020-01-10 00:00:00|\n",
            "|1.6033329963684082|2020-01-13 00:00:00|\n",
            "|1.6033329963684082|2020-01-14 00:00:00|\n",
            "|1.6100000143051147|2020-01-15 00:00:00|\n",
            "+------------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#query to find only all adj closes between Jan 1st, 2020 and March 1st, 2024\n",
        "adj_closes = spark.sql(\"\"\"select `Adj Close`, Date from celh where Date between \"2020-01-01\" and \"2024-03-01\" \"\"\").show(10)\n",
        "adj_closes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh0xBvhhUJL9",
        "outputId": "daa350fb-9cc3-4987-fe34-4e02b8c616fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------------+-------------------+\n",
            "|            Close|               Date|\n",
            "+-----------------+-------------------+\n",
            "|81.62000274658203|2024-02-29 00:00:00|\n",
            "| 79.2699966430664|2024-03-01 00:00:00|\n",
            "|68.41999816894531|2023-09-07 00:00:00|\n",
            "| 67.7699966430664|2024-02-28 00:00:00|\n",
            "| 67.5199966430664|2024-02-27 00:00:00|\n",
            "| 67.3133316040039|2023-09-14 00:00:00|\n",
            "|67.20999908447266|2023-09-08 00:00:00|\n",
            "| 67.0433349609375|2023-09-12 00:00:00|\n",
            "|66.85333251953125|2023-09-13 00:00:00|\n",
            "|66.77999877929688|2023-09-06 00:00:00|\n",
            "+-----------------+-------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#query for closes in descending order between the selected dates\n",
        "biggest_closes = spark.sql(\"\"\"select Close, Date from celh where Date between \"2020-01-01\" and \"2024-03-01\" order by Close desc\"\"\").show(10)\n",
        "biggest_closes"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
